{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename) :\n",
    "    if filename.split('.')[-1] == \"gz\" :\n",
    "        file = gzip.open(filename,'rt')\n",
    "    else :\n",
    "        file = open(filename, 'rt')\n",
    "    return file\n",
    "\n",
    "def collect_target_samples(filename) :\n",
    "    \n",
    "    num_lines = sum(1 for line in open_file(filename))    \n",
    "    data = open_file(filename)\n",
    "    \n",
    "    # collected samples\n",
    "    target_samples = []\n",
    "    \n",
    "    for i, line in tqdm(enumerate(data), total = num_lines) :\n",
    "    \n",
    "        jsondata = json.loads(line)\n",
    "    \n",
    "        if i == 0 :\n",
    "            continue\n",
    "            \n",
    "        context = jsondata['context']\n",
    "        context_len = len(context)\n",
    "        \n",
    "        # preventing sentence split according to . ! ? in answer texts\n",
    "        for q in jsondata['qas'] :\n",
    "\n",
    "            ans =  q['detected_answers'][0]['text']\n",
    "            pos =  q['detected_answers'][0]['char_spans'][0][0]\n",
    "\n",
    "            if '.' in ans :\n",
    "                context = context[:pos] + re.sub('\\.', '♬', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "                \n",
    "            if '!' in ans :\n",
    "                context = context[:pos] + re.sub('!', '♪', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "                \n",
    "            if '?' in ans :\n",
    "                context = context[:pos] + re.sub('\\?', '♩', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "\n",
    "        # split passage to sentences\n",
    "        sen_text = sent_tokenize(context)\n",
    "        sen_pos = []\n",
    "        \n",
    "        sample = dict()\n",
    "        \n",
    "        for j in range(len(sen_text)) :\n",
    "\n",
    "            pos = sum(sen_pos) \n",
    "            pos += len(sen_text[j])\n",
    "                \n",
    "            # adjusting sentence position because sent_tokenize automatically strips splited sentence texts\n",
    "            while pos < context_len and ord(context[pos]) in [10, 32, 160] : \n",
    "                pos += 1\n",
    "\n",
    "            sen_pos.append(pos - sum(sen_pos))\n",
    "\n",
    "            # remove tag texts for improving sentence embedding quality\n",
    "            sen_text[j] = re.sub('\\[TLE\\]', ' ', sen_text[j])\n",
    "            sen_text[j] = re.sub('\\[DOC\\]', ' ', sen_text[j])\n",
    "            sen_text[j] = re.sub('\\[PAR\\]', ' ', sen_text[j])\n",
    "            \n",
    "            #sen_text[j] = re.sub('<P>', ' ', sen_text[j])\n",
    "            #sen_text[j] = re.sub('</P>', ' ', sen_text[j])\n",
    "\n",
    "            # restore replaced characters\n",
    "            sen_text[j] = re.sub('♬', '.', sen_text[j])\n",
    "            sen_text[j] = re.sub('♪', '!', sen_text[j])\n",
    "            sen_text[j] = re.sub('♩', '?', sen_text[j])\n",
    "\n",
    "        sample['sentence_text'] = sen_text\n",
    "        sample['sentence_pos'] = sen_pos\n",
    "            \n",
    "        questions = []\n",
    "        for q in jsondata['qas'] :\n",
    "            \n",
    "            # only use first detected answer\n",
    "            answer = q['detected_answers'][0]\n",
    "            \n",
    "            spans = []\n",
    "            unique_sen = set()\n",
    "            for j in range(len(answer['char_spans'])) :\n",
    "                pos = answer['char_spans'][j][0]\n",
    "                \n",
    "                # find sentence index including target span\n",
    "                idx = 0\n",
    "                for k in range(len(sen_text)) :\n",
    "                    if pos < (sum(sen_pos[:k+1])) :\n",
    "                        idx = k\n",
    "                        break\n",
    "                \n",
    "                # ignoring spans with duplicated sentence index\n",
    "                if idx not in unique_sen :\n",
    "                    spans.append((answer['char_spans'][j], idx))\n",
    "                    unique_sen.add(idx)\n",
    "              \n",
    "            if len(spans) > 1 :\n",
    "                questions.append({\n",
    "                     'qid' : q['qid']\n",
    "                    ,'question' : q['question']\n",
    "                    ,'answer'   : answer['text']\n",
    "                    ,'ans_spans': spans\n",
    "                })\n",
    "            \n",
    "        if len(questions) > 0 :\n",
    "            sample['question'] = questions\n",
    "            target_samples.append(sample)\n",
    "\n",
    "    return target_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['./MRQA-Shared-Task-2019/download_train/HotpotQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/NewsQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz', './MRQA-Shared-Task-2019/download_train/TriviaQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/SearchQA.jsonl.gz'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if False :\n",
    "\n",
    "    train_files = [os.path.join(\"./MRQA-Shared-Task-2019/download_train\", file) for file in os.listdir(\"./MRQA-Shared-Task-2019/download_train\")]\n",
    "    #dev_files = [os.path.join(\"./MRQA-Shared-Task-2019/download_in_domain_dev\", file) for file in os.listdir(\"./MRQA-Shared-Task-2019/download_in_domain_dev\")]\n",
    "    #out_dev_files = [os.path.join(\"./MRQA-Shared-Task-2019/download_out_of_domain_dev\", file) for file in os.listdir(\"./MRQA-Shared-Task-2019/download_out_of_domain_dev\")]\n",
    "\n",
    "    files = train_files\n",
    "    print(files)\n",
    "    \n",
    "    all_target_samples = dict()\n",
    "    for file in files :\n",
    "        print(file)\n",
    "        target_samples = collect_target_samples(file)\n",
    "        questions = []\n",
    "        for l in target_samples :\n",
    "            questions = questions + l['question']\n",
    "        print(\"Num. collected samples :\", len(questions))\n",
    "        \n",
    "        if len(questions) > 0 :\n",
    "            all_target_samples[file] = target_samples\n",
    "\n",
    "    with open(\"all_target_samples_in_domain_train.pickle\", 'wb') as handle:\n",
    "        pickle.dump(all_target_samples, handle)\n",
    "    \n",
    "else :\n",
    "    \n",
    "    with open(\"all_target_samples_in_domain_train.pickle\", 'rb') as handle:\n",
    "        all_target_samples = pickle.load(handle)\n",
    "        \n",
    "print(all_target_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2094/101413 [00:00<00:04, 20937.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/SearchQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101413/101413 [20:44<00:00, 81.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. quesitons : 101413\n",
      "Num. texts     : 854166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [58:36<00:00, 52.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. revised spans : 80665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def correct_target_samples(data) :\n",
    "    \n",
    "    qid = []\n",
    "    span_num = []\n",
    "    spans = []\n",
    "    texts = []\n",
    "    ans = []\n",
    "    for d in tqdm(data) :\n",
    "        sentence_text = d['sentence_text']\n",
    "        for q in d['question'] :\n",
    "            texts.append(q['question'])\n",
    "            spans.append(q['ans_spans'])\n",
    "            span_texts = [sentence_text[t[1]] for t in q['ans_spans']]\n",
    "            texts = texts + span_texts      \n",
    "            qid.append(q['qid'])\n",
    "            ans.append(q['answer'])\n",
    "            span_num.append(len(span_texts))\n",
    "            \n",
    "    print(\"Num. quesitons :\", len(qid))\n",
    "    print(\"Num. texts     :\", len(texts))\n",
    "\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "    vectorizer = TfidfVectorizer().fit(texts)\n",
    "    \n",
    "    correct_spans = []\n",
    "    batch_size = 1000\n",
    "    for i in tqdm(range(0, len(qid), batch_size)) :\n",
    "    \n",
    "        prev_text_num = sum(span_num[:i]) + i\n",
    "        next_text_num = sum(span_num[i:i+batch_size]) + batch_size\n",
    "    \n",
    "        batch_text = texts[prev_text_num : prev_text_num + next_text_num]\n",
    "\n",
    "        text_embed = session.run(embed(batch_text))\n",
    "        text_vect = vectorizer.transform(batch_text)\n",
    "\n",
    "        for j in range(len(span_num[i:i+batch_size])) :\n",
    "            \n",
    "            text_idx = sum(span_num[i:i+j]) + j\n",
    "\n",
    "            # get sentence embedding similarity\n",
    "            q_embed = text_embed[text_idx].reshape(1, -1)\n",
    "            s_embed = text_embed[text_idx+1:text_idx+1+span_num[i+j]].reshape(span_num[i+j], -1)\n",
    "            embed_sim = cosine_similarity(q_embed, s_embed)\n",
    "\n",
    "            # get tfidf vector similarity\n",
    "            q_vect = text_vect[text_idx].reshape(1, -1)\n",
    "            s_vect = text_vect[text_idx+1:text_idx+1+span_num[i+j]].reshape(span_num[i+j], -1)\n",
    "            vect_sim = cosine_similarity(q_vect, s_vect)\n",
    "            \n",
    "            # blend similarity\n",
    "            sim = (embed_sim + vect_sim) / 2\n",
    "        \n",
    "            most_sim_idx = np.argsort(sim)[0][-1]\n",
    "            if most_sim_idx != 0 :\n",
    "                correct_spans.append({\n",
    "                     \"qid\" : qid[i+j]\n",
    "                    ,\"question\" : texts[text_idx]\n",
    "                    ,\"answer\"   : ans[i+j]\n",
    "                    ,\"origin_span\" : (texts[text_idx + 1] , spans[i+j][0])\n",
    "                    ,\"revise_span\" : (texts[text_idx + 1 + most_sim_idx] , spans[i+j][most_sim_idx])\n",
    "                })\n",
    "        \n",
    "    return correct_spans\n",
    "\n",
    "for file in all_target_samples.keys() :\n",
    "    print(file)\n",
    "    correct_spans = correct_target_samples(all_target_samples[file])\n",
    "    print(\"Num. revised spans :\", len(correct_spans))\n",
    "    if len(correct_spans) > 0 :\n",
    "        with open(file + \"_spans.pickle\", 'wb') as handle:\n",
    "            pickle.dump(correct_spans, handle)\n",
    "\n",
    "all_target_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/HotpotQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72929/72929 [00:15<00:00, 4800.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/NewsQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11429/11429 [00:08<00:00, 1302.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18886/18886 [00:04<00:00, 4357.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/TriviaQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61689/61689 [01:01<00:00, 1010.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MRQA-Shared-Task-2019/download_train/SearchQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117385/117385 [01:49<00:00, 1070.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in all_target_samples.keys() :\n",
    "    print(file)\n",
    "    \n",
    "    try :\n",
    "        with open(file + \"_spans.pickle\", 'rb') as handle:\n",
    "            correct_spans = pickle.load(handle)\n",
    "    except :\n",
    "        continue\n",
    "    \n",
    "    # make qid index\n",
    "    qid_index = dict()\n",
    "    for i, row in enumerate(correct_spans) :\n",
    "        qid_index[row['qid']] = i\n",
    "    \n",
    "    path = '/'.join(file.split('/')[:-1])\n",
    "    filename = file.split('/')[-1].split('.')[0]\n",
    "    new_file = open(os.path.join(path, filename + \"_revised.jsonl\"), \"wt\")\n",
    "    \n",
    "    num_lines = sum(1 for line in open_file(file))    \n",
    "    data = open_file(file)\n",
    "    for i, line in tqdm(enumerate(data), total = num_lines) :\n",
    "    \n",
    "        jsondata = json.loads(line)\n",
    "    \n",
    "        if i == 0 :\n",
    "            new_file.write(json.dumps(jsondata) + '\\n')\n",
    "            continue\n",
    "\n",
    "        for j, q in enumerate(jsondata['qas']) :\n",
    "            \n",
    "            if q['qid'] in qid_index :\n",
    "                \n",
    "                revised_idx = 0\n",
    "                revised_span = correct_spans[qid_index[q['qid']]]['revise_span'][1][0]\n",
    "                char_spans = q['detected_answers'][0]['char_spans']\n",
    "                for k in range(1, len(char_spans)) :\n",
    "                    if char_spans[k][0] == revised_span[0] and char_spans[k][1] == revised_span[1] :\n",
    "                        revised_idx = k\n",
    "                        break\n",
    "                        \n",
    "                jsondata['qas'][j]['detected_answers'][0]['token_spans'] = [jsondata['qas'][j]['detected_answers'][0]['token_spans'][revised_idx]]\n",
    "                \n",
    "        new_file.write(json.dumps(jsondata) + '\\n')\n",
    "    \n",
    "    new_file.close()\n",
    "    \n",
    "    with open(os.path.join(path, filename + \"_revised.jsonl\"), 'rt') as f_in:\n",
    "        with gzip.open(os.path.join(path, filename + \"_revised.jsonl\") + '.gz', 'wt') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for file in correct_result.keys() :\n",
    "    for s in correct_result[file] :\n",
    "        data.append([\n",
    "             file\n",
    "            ,s['qid']\n",
    "            ,s['question']\n",
    "            ,s['answer']\n",
    "            ,s['origin_span']\n",
    "            ,s['revise_span']\n",
    "        ])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['dataset', 'qid', 'question', 'answer', 'origin_span', 'revise_span']\n",
    "df.to_csv(\"revise_spans_with_tfidf_out_domain.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(filename) :\n",
    "    \n",
    "    num_lines = sum(1 for line in open_file(filename))    \n",
    "    data = open_file(filename)\n",
    "    \n",
    "    # collected samples\n",
    "    target_samples = []\n",
    "    \n",
    "    for i, line in tqdm(enumerate(data), total = num_lines) :\n",
    "    \n",
    "        jsondata = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72929/72929 [00:06<00:00, 10452.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11429/11429 [00:05<00:00, 2005.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18886/18886 [00:01<00:00, 9657.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriviaQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61689/61689 [00:35<00:00, 1746.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117385/117385 [01:06<00:00, 1761.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in all_target_samples.keys() :\n",
    "    path = '/'.join(file.split('/')[:-1])\n",
    "    filename = file.split('/')[-1].split('.')[0]\n",
    "    print(filename)\n",
    "    test(os.path.join(path, filename + \"_revised.jsonl.gz\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
