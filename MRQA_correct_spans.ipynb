{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename) :\n",
    "    if filename.split('.')[-1] == \"gz\" :\n",
    "        file = gzip.open(filename,'rt')\n",
    "    else :\n",
    "        file = open(filename, 'rt')\n",
    "    return file\n",
    "\n",
    "def collect_target_samples(filename) :\n",
    "    \n",
    "    num_lines = sum(1 for line in open_file(filename))    \n",
    "    data = open_file(filename)\n",
    "    \n",
    "    # collected samples\n",
    "    target_samples = []\n",
    "    \n",
    "    for i, line in tqdm(enumerate(data), total = num_lines) :\n",
    "    \n",
    "        jsondata = json.loads(line)\n",
    "    \n",
    "        if i == 0 :\n",
    "            continue\n",
    "            \n",
    "        context = jsondata['context']\n",
    "        context_len = len(context)\n",
    "        \n",
    "        # preventing sentence split according to . ! ? in answer texts\n",
    "        for q in jsondata['qas'] :\n",
    "\n",
    "            ans =  q['detected_answers'][0]['text']\n",
    "            pos =  q['detected_answers'][0]['char_spans'][0][0]\n",
    "\n",
    "            if '.' in ans :\n",
    "                context = context[:pos] + re.sub('\\.', '♬', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "                \n",
    "            if '!' in ans :\n",
    "                context = context[:pos] + re.sub('!', '♪', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "                \n",
    "            if '?' in ans :\n",
    "                context = context[:pos] + re.sub('\\?', '♩', context[pos : pos + len(ans)]) + context[pos + len(ans):]\n",
    "\n",
    "        # split passage to sentences\n",
    "        sen_text = sent_tokenize(context)\n",
    "        sen_pos = []\n",
    "        \n",
    "        sample = dict()\n",
    "        \n",
    "        for j in range(len(sen_text)) :\n",
    "\n",
    "            pos = sum(sen_pos) \n",
    "            pos += len(sen_text[j])\n",
    "                \n",
    "            # adjusting sentence position because sent_tokenize automatically strips splited sentence texts\n",
    "            while pos < context_len and ord(context[pos]) in [10, 32, 160] : \n",
    "                pos += 1\n",
    "\n",
    "            sen_pos.append(pos - sum(sen_pos))\n",
    "\n",
    "            # remove tag texts for improving sentence embedding quality\n",
    "            sen_text[j] = re.sub('\\[TLE\\]', ' ', sen_text[j])\n",
    "            sen_text[j] = re.sub('\\[DOC\\]', ' ', sen_text[j])\n",
    "            sen_text[j] = re.sub('\\[PAR\\]', ' ', sen_text[j])\n",
    "            \n",
    "            #sen_text[j] = re.sub('<P>', ' ', sen_text[j])\n",
    "            #sen_text[j] = re.sub('</P>', ' ', sen_text[j])\n",
    "\n",
    "            # restore replaced characters\n",
    "            sen_text[j] = re.sub('♬', '.', sen_text[j])\n",
    "            sen_text[j] = re.sub('♪', '!', sen_text[j])\n",
    "            sen_text[j] = re.sub('♩', '?', sen_text[j])\n",
    "\n",
    "        sample['sentence_text'] = sen_text\n",
    "        sample['sentence_pos'] = sen_pos\n",
    "            \n",
    "        questions = []\n",
    "        for q in jsondata['qas'] :\n",
    "            \n",
    "            # only use first detected answer\n",
    "            answer = q['detected_answers'][0]\n",
    "            \n",
    "            spans = []\n",
    "            unique_sen = set()\n",
    "            for j in range(len(answer['char_spans'])) :\n",
    "                pos = answer['char_spans'][j][0]\n",
    "                \n",
    "                # find sentence index including target span\n",
    "                idx = 0\n",
    "                for k in range(len(sen_text)) :\n",
    "                    if pos < (sum(sen_pos[:k+1])) :\n",
    "                        idx = k\n",
    "                        break\n",
    "                \n",
    "                # ignoring spans with duplicated sentence index\n",
    "                if idx not in unique_sen :\n",
    "                    spans.append((answer['char_spans'][j], idx))\n",
    "                    unique_sen.add(idx)\n",
    "              \n",
    "            if len(spans) > 1 :\n",
    "                questions.append({\n",
    "                     'qid' : q['qid']\n",
    "                    ,'question' : q['question']\n",
    "                    ,'answer'   : answer['text']\n",
    "                    ,'ans_spans': spans\n",
    "                })\n",
    "            \n",
    "        if len(questions) > 0 :\n",
    "            sample['question'] = questions\n",
    "            target_samples.append(sample)\n",
    "\n",
    "    return target_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./MRQA-Shared-Task-2019/download_train/NaturalQuestions.jsonl.gz', './MRQA-Shared-Task-2019/download_train/HotpotQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/NewsQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz', './MRQA-Shared-Task-2019/download_train/TriviaQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/SearchQA.jsonl.gz']\n",
      "['./MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestions.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/HotpotQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/NewsQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/SQuAD.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestionsrevised.jsonl', './MRQA-Shared-Task-2019/download_in_domain_dev/SQuADrevised.jsonl', './MRQA-Shared-Task-2019/download_in_domain_dev/TriviaQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/SearchQA.jsonl.gz']\n",
      "./MRQA-Shared-Task-2019/download_train/NaturalQuestions.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104072/104072 [00:31<00:00, 3287.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 0\n",
      "./MRQA-Shared-Task-2019/download_train/HotpotQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72929/72929 [00:32<00:00, 2267.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 26715\n",
      "./MRQA-Shared-Task-2019/download_train/NewsQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11429/11429 [00:17<00:00, 655.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 41\n",
      "./MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18886/18886 [00:06<00:00, 2722.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 598\n",
      "./MRQA-Shared-Task-2019/download_train/TriviaQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61689/61689 [02:02<00:00, 502.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 45983\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestions.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12837/12837 [00:03<00:00, 3380.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 0\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/HotpotQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5905/5905 [00:02<00:00, 2281.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 1681\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/NewsQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 639/639 [00:00<00:00, 709.90it/s]\n",
      "  0%|          | 0/2068 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 0\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/SQuAD.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2068/2068 [00:00<00:00, 2430.85it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3297.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 63\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestionsrevised.jsonl\n",
      "Num. collected samples : 0\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/SQuADrevised.jsonl\n",
      "Num. collected samples : 0\n",
      "./MRQA-Shared-Task-2019/download_in_domain_dev/TriviaQA.jsonl.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7786/7786 [00:15<00:00, 559.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. collected samples : 5860\n",
      "dict_keys(['./MRQA-Shared-Task-2019/download_train/NaturalQuestions.jsonl.gz', './MRQA-Shared-Task-2019/download_train/HotpotQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/NewsQA.jsonl.gz', './MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz', './MRQA-Shared-Task-2019/download_train/TriviaQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestions.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/HotpotQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/NewsQA.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/SQuAD.jsonl.gz', './MRQA-Shared-Task-2019/download_in_domain_dev/NaturalQuestionsrevised.jsonl', './MRQA-Shared-Task-2019/download_in_domain_dev/SQuADrevised.jsonl', './MRQA-Shared-Task-2019/download_in_domain_dev/TriviaQA.jsonl.gz'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if True :\n",
    "\n",
    "    train_files = [os.path.join(\"./MRQA-Shared-Task-2019/download_train\", file) for file in os.listdir(\"./MRQA-Shared-Task-2019/download_train\")]\n",
    "    dev_files = [os.path.join(\"./MRQA-Shared-Task-2019/download_in_domain_dev\", file) for file in os.listdir(\"./MRQA-Shared-Task-2019/download_in_domain_dev\")]\n",
    "\n",
    "    print(train_files)\n",
    "    print(dev_files)\n",
    "\n",
    "    all_target_samples = dict()\n",
    "    for file in train_files[:-1] + dev_files[:-1] :\n",
    "        print(file)\n",
    "        target_samples = collect_target_samples(file)\n",
    "        all_target_samples[file] = target_samples\n",
    "        questions = []\n",
    "        for l in target_samples :\n",
    "            questions = questions + l['question']\n",
    "        print(\"Num. collected samples :\", len(questions))\n",
    "\n",
    "    with open(\"all_target_samples.pickle\", 'wb') as handle:\n",
    "        pickle.dump(all_target_samples, handle)\n",
    "    \n",
    "else :\n",
    "    \n",
    "    with open(\"all_target_samples.pickle\", 'rb') as handle:\n",
    "        all_target_samples = pickle.load(handle)\n",
    "        \n",
    "print(all_target_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. quesitons : 598\n",
      "Num. texts     : 2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:15<00:00,  3.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def correct_target_samples(data) :\n",
    "    \n",
    "    qid = []\n",
    "    span_num = []\n",
    "    spans = []\n",
    "    texts = []\n",
    "    ans = []\n",
    "    for d in data :\n",
    "        sentence_text = d['sentence_text']\n",
    "        for q in d['question'] :\n",
    "            texts.append(q['question'])\n",
    "            spans.append(q['ans_spans'])\n",
    "            span_texts = [sentence_text[t[1]] for t in q['ans_spans']]\n",
    "            texts = texts + span_texts      \n",
    "            qid.append(q['qid'])\n",
    "            ans.append(q['answer'])\n",
    "            span_num.append(len(span_texts))\n",
    "            \n",
    "    print(\"Num. quesitons :\", len(qid))\n",
    "    print(\"Num. texts     :\", len(texts))\n",
    "\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "    batch_size = 512\n",
    "    text_embed = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)) :\n",
    "        text_embed.append(session.run(embed(texts[i:i+batch_size])))\n",
    "    text_embed = np.concatenate(text_embed)\n",
    "    #text_embed = np.zeros((len(texts), 512))\n",
    "        \n",
    "    correct_spans = []\n",
    "    for i in range(len(qid)) :\n",
    "        # get accumulated previous text number\n",
    "        acc_num = sum(span_num[:i]) + i\n",
    "        q_embed = text_embed[acc_num].reshape(-1, 512)\n",
    "        s_embed = text_embed[acc_num+1:acc_num+1+span_num[i]].reshape(-1, 512)\n",
    "        sim = cosine_similarity(q_embed, s_embed)\n",
    "        most_sim_idx = np.argsort(sim)[0][-1]\n",
    "        if most_sim_idx != 0 :\n",
    "            correct_spans.append({\n",
    "                 \"qid\" : qid[i]\n",
    "                ,\"question\" : texts[acc_num]\n",
    "                ,\"answer\"   : ans[i]\n",
    "                ,\"origin_span\" : (texts[acc_num + 1] , spans[i][0])\n",
    "                ,\"revise_span\" : (texts[acc_num + 1 + most_sim_idx] , spans[i][most_sim_idx])\n",
    "            })\n",
    "        \n",
    "    return correct_spans\n",
    "\n",
    "correct_spans = correct_target_samples(all_target_samples['./MRQA-Shared-Task-2019/download_train/SQuAD.jsonl.gz'].copy())\n",
    "len(correct_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "\n",
    "for s in correct_spans :\n",
    "    data.append([\n",
    "         s['qid']\n",
    "        ,s['question']\n",
    "        ,s['answer']\n",
    "        ,s['origin_span']\n",
    "        ,s['revise_span']\n",
    "    ])\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['qid', 'question', 'answer', 'origin_span', 'revise_span']\n",
    "df.to_csv(\"revise_spans.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
